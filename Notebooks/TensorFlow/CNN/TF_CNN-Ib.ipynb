{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Neural Networks and Deep Learning</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\"> Predictive Analysis - Image Processing</h2>\n",
    "\n",
    "[source](https://medium.com/swlh/classifying-fashion-mnist-dataset-with-convolutional-neural-nets-dd092d755164)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip3 install -U opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 11:58:05.794025: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:05.794082: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "X_Train (60000, 28, 28)\n",
      "Y_Train (60000,)\n",
      "X_Test (10000, 28, 28)\n",
      "Y_Test (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print('Shape:')\n",
    "print('X_Train {}'.format(x_train.shape))\n",
    "print('Y_Train {}'.format(y_train.shape))\n",
    "print('X_Test {}'.format(x_test.shape))\n",
    "print('Y_Test {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_samples(sample, plot=True):\n",
    "    counter = {}\n",
    "    for key in sample:\n",
    "        if key not in counter.keys():\n",
    "            counter[key] = 1\n",
    "        else:\n",
    "            counter[key] = counter[key] + 1\n",
    "    df_dict = {'cat':[x for x in counter.keys()], \n",
    "               'cnt':[y for y in  counter.values()]}\n",
    "    cnt_df = pd.DataFrame(df_dict)\n",
    "    if plot:\n",
    "        sns.barplot(data=cnt_df, x='cat', y='cnt')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('# of Obs. in each Category')\n",
    "        return None\n",
    "    else:\n",
    "        return cnt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_samples(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Image\n",
    "def scale_img(dim, img):\n",
    "    resized_img = cv2.resize(np.asarray(img), dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized_img\n",
    "\n",
    "#Normalize Image\n",
    "def normalize_img(img, is_gray = False):\n",
    "    normalized_img = cv2.normalize(img, img, 0, 255, cv2.NORM_MINMAX)\n",
    "    if is_gray:\n",
    "        cvt = cv2.cvtColor(normalized_img, cv2.COLOR_BGR2GRAY)\n",
    "        return cvt\n",
    "    return normalized_img\n",
    "\n",
    "#Rotate Image\n",
    "def rotate_img(img, rot_deg):\n",
    "    rows,cols = img.shape[0], img.shape[1]\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),rot_deg,1)\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return dst\n",
    "\n",
    "#Translate Image\n",
    "def translate_img(img,x=100,y=50):\n",
    "    rows,cols = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1,0,x],[0,1,y]])\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "X_Train (180000, 28, 28, 1)\n",
      "Y_Train (180000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train_tran = x_train.tolist()\n",
    "y_train_tran = y_train.tolist()\n",
    "ctr = 0\n",
    "for img in x_train:\n",
    "    x_train_tran.append(rotate_img(img, 90))\n",
    "    y_train_tran.append(y_train[ctr])\n",
    "    x_train_tran.append(translate_img(img, x=15, y=15))\n",
    "    y_train_tran.append(y_train[ctr])\n",
    "    ctr = ctr + 1\n",
    "    \n",
    "x_train_tran = np.array(x_train_tran).reshape(len(x_train_tran), 28, 28, 1)\n",
    "y_train_tran = tf.keras.utils.to_categorical(np.array(y_train_tran))\n",
    "#x_train_tran, y_train_tran = shuffle(x_train_tran, y_train_tran)\n",
    "\n",
    "print('Shape:')\n",
    "print('X_Train {}'.format(x_train_tran.shape))\n",
    "print('Y_Train {}'.format(y_train_tran.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 11:58:14.063771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-12 11:58:14.065190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.065346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.065481: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.065614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.065748: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.065876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.066005: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.066135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/renato/Documents/env_default/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-12 11:58:14.066153: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-02-12 11:58:14.066989: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cnn_model = tf.keras.Sequential()\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), activation='relu',input_shape=(28,28,1)))\n",
    "cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "cnn_model.add(tf.keras.layers.Flatten())\n",
    "cnn_model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "cnn_model.add(tf.keras.layers.Dense(tf.keras.utils.to_categorical(y_train).shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 11:58:14.165703: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1128960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625/5625 [==============================] - 65s 11ms/step - loss: 0.9149 - accuracy: 0.6577\n",
      "Epoch 2/150\n",
      "5625/5625 [==============================] - 68s 12ms/step - loss: 0.7128 - accuracy: 0.7224\n",
      "Epoch 3/150\n",
      "5625/5625 [==============================] - 71s 13ms/step - loss: 0.6567 - accuracy: 0.7408\n",
      "Epoch 4/150\n",
      "5625/5625 [==============================] - 67s 12ms/step - loss: 0.6295 - accuracy: 0.7511\n",
      "Epoch 5/150\n",
      "5625/5625 [==============================] - 63s 11ms/step - loss: 0.6063 - accuracy: 0.7613\n",
      "Epoch 6/150\n",
      "5625/5625 [==============================] - 65s 11ms/step - loss: 0.5929 - accuracy: 0.7655\n",
      "Epoch 7/150\n",
      "5625/5625 [==============================] - 65s 12ms/step - loss: 0.5801 - accuracy: 0.7711\n",
      "Epoch 8/150\n",
      "5625/5625 [==============================] - 63s 11ms/step - loss: 0.5687 - accuracy: 0.7753\n",
      "Epoch 9/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5612 - accuracy: 0.7782\n",
      "Epoch 10/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5547 - accuracy: 0.7812\n",
      "Epoch 11/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5489 - accuracy: 0.7826\n",
      "Epoch 12/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5454 - accuracy: 0.7835\n",
      "Epoch 13/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5407 - accuracy: 0.7857\n",
      "Epoch 14/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5377 - accuracy: 0.7866\n",
      "Epoch 15/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.5319 - accuracy: 0.7880\n",
      "Epoch 16/150\n",
      "5625/5625 [==============================] - 68s 12ms/step - loss: 0.5295 - accuracy: 0.7897\n",
      "Epoch 17/150\n",
      "5625/5625 [==============================] - 64s 11ms/step - loss: 0.5239 - accuracy: 0.7917\n",
      "Epoch 18/150\n",
      "5625/5625 [==============================] - 65s 12ms/step - loss: 0.5237 - accuracy: 0.7930\n",
      "Epoch 19/150\n",
      "5625/5625 [==============================] - 66s 12ms/step - loss: 0.5219 - accuracy: 0.7929\n",
      "Epoch 20/150\n",
      "5625/5625 [==============================] - 77s 14ms/step - loss: 0.5171 - accuracy: 0.7941\n",
      "Epoch 21/150\n",
      "5625/5625 [==============================] - 72s 13ms/step - loss: 0.5139 - accuracy: 0.7957\n",
      "Epoch 22/150\n",
      "5625/5625 [==============================] - 78s 14ms/step - loss: 0.5133 - accuracy: 0.7957\n",
      "Epoch 23/150\n",
      "5625/5625 [==============================] - 70s 12ms/step - loss: 0.5130 - accuracy: 0.7970\n",
      "Epoch 24/150\n",
      "5625/5625 [==============================] - 67s 12ms/step - loss: 0.5079 - accuracy: 0.7970\n",
      "Epoch 25/150\n",
      "5625/5625 [==============================] - 123s 22ms/step - loss: 0.5106 - accuracy: 0.7972\n",
      "Epoch 26/150\n",
      "5625/5625 [==============================] - 71s 13ms/step - loss: 0.5060 - accuracy: 0.7991\n",
      "Epoch 27/150\n",
      "5625/5625 [==============================] - 69s 12ms/step - loss: 0.5026 - accuracy: 0.7995\n",
      "Epoch 28/150\n",
      "5625/5625 [==============================] - 69s 12ms/step - loss: 0.5029 - accuracy: 0.8000\n",
      "Epoch 29/150\n",
      "5625/5625 [==============================] - 67s 12ms/step - loss: 0.5010 - accuracy: 0.7997\n",
      "Epoch 30/150\n",
      "5625/5625 [==============================] - 64s 11ms/step - loss: 0.5003 - accuracy: 0.8007\n",
      "Epoch 31/150\n",
      "5625/5625 [==============================] - 64s 11ms/step - loss: 0.4980 - accuracy: 0.8020\n",
      "Epoch 32/150\n",
      "5625/5625 [==============================] - 64s 11ms/step - loss: 0.5012 - accuracy: 0.8014\n",
      "Epoch 33/150\n",
      "5625/5625 [==============================] - 68s 12ms/step - loss: 0.4964 - accuracy: 0.8020\n",
      "Epoch 34/150\n",
      "5625/5625 [==============================] - 69s 12ms/step - loss: 0.4965 - accuracy: 0.8019\n",
      "Epoch 35/150\n",
      "5625/5625 [==============================] - 65s 12ms/step - loss: 0.4935 - accuracy: 0.8039\n",
      "Epoch 36/150\n",
      "5625/5625 [==============================] - 66s 12ms/step - loss: 0.4934 - accuracy: 0.8051\n",
      "Epoch 37/150\n",
      "5625/5625 [==============================] - 68s 12ms/step - loss: 0.4907 - accuracy: 0.8047\n",
      "Epoch 38/150\n",
      "5625/5625 [==============================] - 63s 11ms/step - loss: 0.4878 - accuracy: 0.8061\n",
      "Epoch 39/150\n",
      "5625/5625 [==============================] - 64s 11ms/step - loss: 0.4881 - accuracy: 0.8053\n",
      "Epoch 40/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4847 - accuracy: 0.8069\n",
      "Epoch 41/150\n",
      "5625/5625 [==============================] - 61s 11ms/step - loss: 0.4871 - accuracy: 0.8067\n",
      "Epoch 42/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4845 - accuracy: 0.8074\n",
      "Epoch 43/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4861 - accuracy: 0.8074\n",
      "Epoch 44/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4827 - accuracy: 0.8084\n",
      "Epoch 45/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4862 - accuracy: 0.8069\n",
      "Epoch 46/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4783 - accuracy: 0.8103\n",
      "Epoch 47/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4796 - accuracy: 0.8081\n",
      "Epoch 48/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4830 - accuracy: 0.8086\n",
      "Epoch 49/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4781 - accuracy: 0.8096\n",
      "Epoch 50/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4769 - accuracy: 0.8108\n",
      "Epoch 51/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4826 - accuracy: 0.8094\n",
      "Epoch 52/150\n",
      "5625/5625 [==============================] - 62s 11ms/step - loss: 0.4749 - accuracy: 0.8112\n",
      "Epoch 53/150\n",
      "5625/5625 [==============================] - 70s 12ms/step - loss: 0.4763 - accuracy: 0.8117\n",
      "Epoch 54/150\n",
      "5625/5625 [==============================] - 71s 13ms/step - loss: 0.4771 - accuracy: 0.8106\n",
      "Epoch 55/150\n",
      "5625/5625 [==============================] - 68s 12ms/step - loss: 0.4838 - accuracy: 0.8092\n",
      "Epoch 56/150\n",
      "5020/5625 [=========================>....] - ETA: 6s - loss: 0.4727 - accuracy: 0.8112"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-4, patience=15, verbose=2, mode='auto')\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(x_train_tran, y_train_tran, batch_size=32, epochs=150, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cnn_model.predict(x_test_tran)\n",
    "pred_list = []\n",
    "for i in preds:\n",
    "    pred_list.append(np.argmax(i))\n",
    "\n",
    "print('{}%'.format(accuracy_score(y_pred=pred_list,y_true=y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {0:'T-shirt/top',\n",
    "             1:'Trouser',\n",
    "             2:'Pullover',\n",
    "             3:'Dress',\n",
    "             4:'Coat',\n",
    "             5:'Sandal',\n",
    "             6:'Shirt',\n",
    "             7:'Sneaker',\n",
    "             8:'Bag',\n",
    "             9:'Ankle boot'}\n",
    "\n",
    "true_class_ct = {}\n",
    "\n",
    "for i in range(10):\n",
    "    true_class_ct[class_map[i]] = sum(1 for x in incorrect_df['true'] if x == class_map[i])\n",
    "    pred_class_ct = {}\n",
    "for i in range(10):\n",
    "    pred_class_ct[class_map[i]] = sum(1 for x in incorrect_df['pred'] if x == class_map[i])\n",
    "    \n",
    "pred_class_ctop = {}\n",
    "\n",
    "def percentage_error(trueval, changedval, i):\n",
    "    op[class_map[i]] = ((changedval - trueval)/trueval) * 100\n",
    "    \n",
    "for ct in range(10):\n",
    "    tv = true_class_ct[class_map[ct]]\n",
    "    cv = pred_class_ct[class_map[ct]]\n",
    "    percentage_error(tv, cv, ct)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "width = 0.4\n",
    "x = np.arange(10)\n",
    "rects1 = ax.bar(x - width/2, list(true_class_ct.values()), width, label='True')\n",
    "rects2 = ax.bar(x + width/2, list(pred_class_ct.values()), width, label='CNN Predicted')\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Classification Error in each class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(list(op.keys()))\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=list(op.keys()), y=list(op.values()))\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title('Percentage error for each class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sneakers seems to be a problem. The model classified 60% more samples as sneakers than the original samples that were sneakers. Percentage, however, by itself is not enough. If we consider both bar charts, we can see that our model is overfitting few categories. There are few ways to avoid that.\n",
    "\n",
    "+ Regularization (L1/L2)\n",
    "+ Adding Dropout Layers\n",
    "+ Adding more data by data augmentation (which we did).\n",
    "\n",
    "Model fitting in my opinion is an iterative process. You begin with a model that you think has best hyperparameters to understand data. You then assess the model to see how well it performs on the training data using metrics like accuracy, loss etc. If the model seems to be not doing great, you tweak the hyperparameters and fit the model again until you reach a desired model. Hypothetically, a desired model is the one that has close to 100% accuracy and, that does not over or under-fit on training set and is not biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
