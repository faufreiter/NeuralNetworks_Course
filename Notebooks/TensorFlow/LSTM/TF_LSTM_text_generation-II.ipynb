{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Neural Networks and Deep Learning</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">RNN, LSTM and GRUs</h2>\n",
    "\n",
    "using Tensor Flow and Keras\n",
    "\n",
    "Based on this [post](https://blog.quantinsti.com/rnn-lstm-gru-trading/?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start off by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"../../../data/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144987"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text.find('happy summer days.\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = raw_text[:145000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = raw_text[raw_text.find('\\n\\n\\n\\n'):]\n",
    "raw_text = re.sub(r'[\\n ]+',' ', raw_text)\n",
    "\n",
    "import string\n",
    "for char in string.punctuation:\n",
    "    raw_text = raw_text.replace(char, ' ')\n",
    "\n",
    "raw_text = raw_text.replace('  ', ' ')\n",
    "raw_text = raw_text.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the list of unique sorted lowercase characters in the book is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '0': 1,\n",
       " '3': 2,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 5,\n",
       " 'd': 6,\n",
       " 'e': 7,\n",
       " 'f': 8,\n",
       " 'g': 9,\n",
       " 'h': 10,\n",
       " 'i': 11,\n",
       " 'j': 12,\n",
       " 'k': 13,\n",
       " 'l': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'o': 17,\n",
       " 'p': 18,\n",
       " 'q': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  135172\n",
      "Total Vocab:  29\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the book has just under 150,000 characters and that when converted to lowercase that there are only 37 distinct characters in the vocabulary for the network to learn. Much more than the 26 in the alphabet.\n",
    "\n",
    "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course).\n",
    "\n",
    "For example, if the sequence length is 5 (for simplicity) then the first two training patterns would be as follows:  \n",
    "\n",
    "CHAPT -> E\n",
    "HAPTE -> R\n",
    "\n",
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  135072\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code to this point shows us that when we split up the dataset into training data for the network to learn that we have just under 150,000 training pattens. This makes sense as excluding the first 100 characters, we have one training pattern to predict each of the remaining characters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n",
    "\n",
    "We can implement these steps as below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = tf.keras.utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with 47 classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 12:38:04.232935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:04.359980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:04.360130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:04.361331: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-19 12:38:04.361851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:04.361990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:04.362105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:05.440775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:05.440963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:05.441100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-19 12:38:05.441688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10807 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our model to the data. Here we use a modest number of 20/30 epochs and a large batch size of 32/64/128 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 12:38:08.273797: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4219/4221 [============================>.] - ETA: 0s - loss: 2.6448\n",
      "Epoch 1: loss improved from inf to 2.64476, saving model to weights-improvement-01-2.6448.hdf5\n",
      "4221/4221 [==============================] - 26s 6ms/step - loss: 2.6448\n",
      "Epoch 2/10\n",
      "4218/4221 [============================>.] - ETA: 0s - loss: 2.4175\n",
      "Epoch 2: loss improved from 2.64476 to 2.41737, saving model to weights-improvement-02-2.4174.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 2.4174\n",
      "Epoch 3/10\n",
      "4219/4221 [============================>.] - ETA: 0s - loss: 2.2765\n",
      "Epoch 3: loss improved from 2.41737 to 2.27651, saving model to weights-improvement-03-2.2765.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 2.2765\n",
      "Epoch 4/10\n",
      "4213/4221 [============================>.] - ETA: 0s - loss: 2.1664\n",
      "Epoch 4: loss improved from 2.27651 to 2.16621, saving model to weights-improvement-04-2.1662.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 2.1662\n",
      "Epoch 5/10\n",
      "4221/4221 [==============================] - ETA: 0s - loss: 2.0806\n",
      "Epoch 5: loss improved from 2.16621 to 2.08057, saving model to weights-improvement-05-2.0806.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 2.0806\n",
      "Epoch 6/10\n",
      "4219/4221 [============================>.] - ETA: 0s - loss: 2.0105\n",
      "Epoch 6: loss improved from 2.08057 to 2.01055, saving model to weights-improvement-06-2.0105.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 2.0105\n",
      "Epoch 7/10\n",
      "4214/4221 [============================>.] - ETA: 0s - loss: 1.9493\n",
      "Epoch 7: loss improved from 2.01055 to 1.94913, saving model to weights-improvement-07-1.9491.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 1.9491\n",
      "Epoch 8/10\n",
      "4216/4221 [============================>.] - ETA: 0s - loss: 1.8948\n",
      "Epoch 8: loss improved from 1.94913 to 1.89476, saving model to weights-improvement-08-1.8948.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 1.8948\n",
      "Epoch 9/10\n",
      "4215/4221 [============================>.] - ETA: 0s - loss: 1.8459\n",
      "Epoch 9: loss improved from 1.89476 to 1.84587, saving model to weights-improvement-09-1.8459.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 1.8459\n",
      "Epoch 10/10\n",
      "4218/4221 [============================>.] - ETA: 0s - loss: 1.7973\n",
      "Epoch 10: loss improved from 1.84587 to 1.79741, saving model to weights-improvement-10-1.7974.hdf5\n",
      "4221/4221 [==============================] - 24s 6ms/step - loss: 1.7974\n",
      "training took 4.032660667101542 minutes\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "history = model.fit(X, y, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_list, \n",
    "                    verbose=1)\n",
    "\n",
    "print(f\"training took {(time.time() - t0)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network loss decreased almost every epoch and I expect the network could benefit from training for many more epochs.\n",
    "\n",
    "In the next section we will look at using this model to generate new text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see different results because of the stochastic nature of the model, and because it is hard to fix the random seed for LSTM models to get 100% reproducible results. This is not a concern for this generative model.\n",
    "\n",
    "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
    "\n",
    "You can delete them all except the one with the smallest loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [x for x in os.listdir() if x.endswith('hdf5')]\n",
    "\n",
    "loss = 9.9999\n",
    "chosen = ''\n",
    "for filename in filenames:\n",
    "    pattern = re.search(\"([0-9]{2})(\\-)([0-9]{1}\\.{1}[0-9]{4})(\\.)\",filename)\n",
    "    if pattern:\n",
    "        newloss = float(pattern.group(3).strip('.')) \n",
    "    if newloss < loss:\n",
    "        loss = newloss\n",
    "        chosen = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the suboptimal weight files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.remove(chosen)\n",
    "for filename in filenames:\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text with an LSTM Network\n",
    "\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "model.load_weights(chosen)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to actually make predictions.\n",
    "\n",
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example first outputs the selected random seed, then each character as it is generated.\n",
    "\n",
    "For example, below are the results from one run of this text generator. The random seed was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ain so that altogether for the first minute or two it was as much as she could do to hold it as soon \"\n",
      " alice she had been to the bage wou d date said the ming and the horpe and the harter was soie and the march hare said to herself a soting to said the katter a tore and the said the karter and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice was soiee and alice w\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note some observations about the generated text.  \n",
    "+ It generally conforms to the line format observed in the original text of less than 80 characters before a new line.\n",
    "+ The characters are separated into word-like groups and most groups are actual English words (e.g. “the”, “little” and “was”), but many do not (e.g. “lott”, “tiie” and “taede”).\n",
    "+ Some of the words in sequence make sense(e.g. “and the white rabbit“), but many do not (e.g. “wese tilel“).\n",
    "\n",
    "The fact that this character based model of the book produces output like this is very impressive. It gives you a sense of the learning capabilities of LSTM networks.\n",
    "\n",
    "The results are not perfect. In the next section we look at improving the quality of results by developing a much larger LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger LSTM Recurrent Neural Network\n",
    "\n",
    "We got results, but not excellent results in the previous section. Now, we can try to improve the quality of the generated text by creating a much larger network.\n",
    "\n",
    "We will keep the number of memory units the same at 256, but add a second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.LSTM(256))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also change the filename of the checkpointed weights so that we can tell the difference between weights for this network and the previous (by appending the word “bigger” in the filename)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will increase the number of training epochs from 20 to 50 and use batch size 32/64 to give the network more of an opportunity to be updated and learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4220/4221 [============================>.] - ETA: 0s - loss: 2.4439\n",
      "Epoch 1: loss improved from inf to 2.44382, saving model to weights-improvement-01-2.4438-bigger.hdf5\n",
      "4221/4221 [==============================] - 57s 13ms/step - loss: 2.4438\n",
      "Epoch 2/10\n",
      "4218/4221 [============================>.] - ETA: 0s - loss: 2.0136\n",
      "Epoch 2: loss improved from 2.44382 to 2.01348, saving model to weights-improvement-02-2.0135-bigger.hdf5\n",
      "4221/4221 [==============================] - 56s 13ms/step - loss: 2.0135\n",
      "Epoch 3/10\n",
      "4218/4221 [============================>.] - ETA: 0s - loss: 1.8288\n",
      "Epoch 3: loss improved from 2.01348 to 1.82864, saving model to weights-improvement-03-1.8286-bigger.hdf5\n",
      "4221/4221 [==============================] - 56s 13ms/step - loss: 1.8286\n",
      "Epoch 4/10\n",
      " 945/4221 [=====>........................] - ETA: 44s - loss: 1.7432"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                                monitor='loss', \n",
    "                                                verbose=1, \n",
    "                                                save_best_only=True, \n",
    "                                                mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "history = model.fit(X, y, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "print(f\"training took {(time.time() - t0)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [x for x in os.listdir() if x.endswith('bigger.hdf5')]\n",
    "\n",
    "loss = 9.9999\n",
    "chosen = ''\n",
    "for filename in filenames:\n",
    "    pattern = re.search(\"([0-9]{2})(\\-)([0-9]{1}\\.{1}[0-9]{4})(\\-bigger)\",filename)\n",
    "    if pattern:\n",
    "        newloss = float(pattern.group(3).strip('.')) \n",
    "    if newloss < loss:\n",
    "        loss = newloss\n",
    "        chosen = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the suboptimal weight files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.remove(chosen)\n",
    "for filename in filenames:\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "model.load_weights(chosen)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of running this text generation script produces the output below.\n",
    "\n",
    "The randomly chosen seed text was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that generally there are fewer spelling mistakes and the text looks more realistic, but is still quite nonsensical.\n",
    "\n",
    "For example the same phrases get repeated again and again. Quotes are opened but not closed.\n",
    "\n",
    "These are better results but there is still a lot of room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Extension Ideas to Improve the Model\n",
    "\n",
    "Below are 10 ideas that may further improve the model that you could experiment with are:\n",
    "\n",
    "  + Predict fewer than 1,000 characters as output for a given seed.\n",
    "  + Remove all punctuation from the source text, and therefore from the models’ vocabulary.\n",
    "  + Try a one hot encoded for the input sequences.\n",
    "  + Train the model on padded sentences rather than random sequences of characters.\n",
    "  + Increase the number of training epochs to 100 or many hundreds.\n",
    "  + Add dropout to the visible input layer and consider tuning the dropout percentage.\n",
    "  + Tune the batch size, try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
    "  + Add more memory units to the layers and/or more layers.\n",
    "  + Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
    "  + Change the LSTM layers to be “stateful” to maintain state across batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "This character text model is a popular way for generating text using recurrent neural networks.\n",
    "\n",
    "Below are some more resources and tutorials on the topic if you are interested in going deeper. Perhaps the most popular is the tutorial by Andrej Karpathy titled “The Unreasonable Effectiveness of Recurrent Neural Networks“.\n",
    "\n",
    "+ [Generating Text with Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf)\n",
    "+ [Keras code example of LSTM for text generation](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)\n",
    "+ [Lasagne code example of LSTM for text generation](https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py)  \n",
    "+ [MXNet tutorial for using an LSTM for text generation](http://mxnetjl.readthedocs.io/en/latest/tutorial/char-lstm.html)\n",
    "+ [Auto-Generating Clickbait With Recurrent Neural Networks](https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)\n",
    "\n",
    "#### Summary\n",
    "\n",
    "In this notebook you discovered how you can develop an LSTM recurrent neural network for text generation in Python with the Keras deep learning library.\n",
    "\n",
    "+ Where to download the ASCII text for classical books for free that you can use for training.\n",
    "+ How to train an LSTM network on text sequences and how to use the trained network to generate new sequences.\n",
    "+ How to develop stacked LSTM networks and lift the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
